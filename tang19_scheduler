#!/usr/bin/python3

"""
Scheduler for Tang19 Model builds
"""

import json
from math import log10
import subprocess
import sys

base_config = {
	"name"          : None,
	"errfile"       : None,
	"outfile"       : None,
	"alpha"         : None,
	"tol"           : 1e-3,
	"trainum"       : None,
	"val"           : -1,
	"batchsize"     : 1200,
	"data"          : "data/nmrshift_Kang20filter_training.pickle.xz",
	"kernel"        : "Tang19",
	"mem"           : 15000,
	"gpu_mem"       : 3500,
	"slurm_outs"    : "slurm_outs",
	"save"          : "built_tang19_models",
	"shuffle"       : True,
	"h_elm"         : 0.3,
	"h_elm_range"   : (1e-7, 0.90),
	"l_scale"       : 0.05,
	"l_scale_range" : (1e-7, 0.90),
	"starting_prob" : 250.0,
	"starting_range": (1e-7, 500),
	"stopping_prob" : 0.05,
	"stopping_range": (1e-7, 0.90),
	"optimize"      : False,
	"atom"          : 'C'
}

configs = []
resample = 1
sizes = [350, 1000, 2000, 4000]
apas  = [1e-8, 1e-4, 1e-1]

base_name = "tang19_build_"

for n in sizes:
	num = str(int(n))
	for alpha in apas:
		asize = -log10(alpha)
		asize = str(int(asize))
		if resample == 1:
			name = f"{base_name}{num}_{asize}"
			config = base_config.copy()
			config["name"] = name+'.pickle'
			config["errfile"] = name+'.err'
			config["outfile"] = name+'.out'
			config['alpha'] = alpha
			config['trainum'] = n
			configs.append(config)
			continue			
		else:
			for r in range(1,resample+1):
				name = f"{base_name}{num}_{asize}_r{str(r)}"
				config = base_config.copy()
				config["name"] = name+'.pickle'
				config["errfile"] = name+'.err'
				config["outfile"] = name+'.out'
				config['alpha'] = alpha
				config['trainum'] = n
				configs.append(config)

print(json.dumps(configs, indent=2))

# 		cmd = ''.join((cmd_1,
# 				f" -e {c['errfile']}",
# 				f" -o {c['outfile']}",
# 				f" -m {c['mem']}",
# 				f" -g {c['gpu_mem']}",
# 				f" -l {c['slurm_outs']}"))
# 		
# 		print(cmd)
# print(cmd)
# 	cmd_list = cmd.split()
# 	print(cmd_list)
# 	retval = subprocess.check_output(cmd_list)
# 	result = retval.decode('utf-8')
# 	print(result,end="")
# """
# arg        = parser.parse_args()
# n          = int(arg.train)
# tol        = float(arg.tol)
# chunk_size = int(arg.batch)
# out_path   = str(arg.out)
# 
# df = pd.read_pickle(arg.data,compression='xz')
# 
# print(df.shape)
# 
# for i in range(0, df.shape[0], n):
# 	if i+n < df.shape[0]: print(i,i+n)
# 	else:                 print(i, df.shape[0])
# 	
# df[:i], df[i+n:]
# 
# n = 1000
# tot 19347
# 19 diff models
# n = 1800
# be 18
# whatever is left over, make a new file for remainder_n.pickle.xz
# 
# bgpr_n_1.model.pickle
# bgpr_1000_1.model.pickle
# bgpr_1000_2.model.pickle
# 
# parser.add_argument('--data', '-d', required=True, type=str,
# 	metavar='<str>', help='xz zipped pickled pandas dataframe')
# parser.add_argument('--train', '-n', required=True,
# 	metavar='<int>', help='training set size')
# parser.add_argument('--batch', '-b', required=True,
# 	metavar='<int>', help='batch size')
# parser.add_argument('--tol', '-t', required=False, default=1e-3,
# 	metavar='<float>', help='tolerance for optimization')
# parser.add_argument('--out', '-o', required=True,
# 	metavar='<str>', help='directory for saved models')
# 
# 
# """